{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da395a8c-3d76-4cae-8c2f-8cfab3d8a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessed data shape: x: (18213, 1, 128, 86), y: (18213,).\n",
      "x dtype: float32, y dtype: int64\n",
      "x range: (0.0, 1.0), y range: (0, 1)\n",
      "Loading the validation data from: /Users/zaherqubein/Desktop/Project/Validation_Set\n",
      "Loaded data from /Users/zaherqubein/Desktop/Project/Validation_Set/val_all.h5\n",
      "Number of validation files: 41\n",
      "Training data path: /Users/zaherqubein/Desktop/Project/Training_Set\n",
      "Validation data path: /Users/zaherqubein/Desktop/Project/Validation_Set\n",
      "Training data shape: torch.Size([18213, 1, 128, 86])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|                                       | 0/570 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(train_x, train_y)\n\u001b[1;32m    305\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 307\u001b[0m train(model, train_loader, criterion, optimizer, scheduler, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    309\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, scheduler, device, num_epochs, patience)\u001b[0m\n\u001b[1;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m#compute loss and update model parameters\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    169\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    170\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import csv\n",
    "\n",
    "#paths to our training/validation \n",
    "train_path = \"/Users/zaherqubein/Desktop/Project/Training_Set\"\n",
    "val_path = \"/Users/zaherqubein/Desktop/Project/Validation_Set\"\n",
    "\n",
    "class AudioResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AudioResNet, self).__init__()\n",
    "        #set number of input channels to 128\n",
    "        self.in_channels = 128\n",
    "        #first conv network\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.layer1 = self.make_layer(128, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(256, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(512, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(1024, 2, stride=2)\n",
    "        #reduce the pooling sieze on the feature map to 1 on 1\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#create a ResNet Layer with multiple Blocks\n",
    "    def make_layer(self, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(self.make_resNET_block(out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(self.make_resNET_block(out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_resNET_block(self, out_channels, stride):\n",
    "        downsample = None\n",
    "        #check if we need to do downsample by checking if output channels\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "            #create a ResBlock update number of input channels\n",
    "        block = ResBlock(self.in_channels, out_channels, stride, downsample)\n",
    "        self.in_channels = out_channels\n",
    "        return block\n",
    "        #in this function we pass it to the next layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pass the input between our convo layers we have 4\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #apply pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "    #then pass it through Fully conected lauer amd apply sigmoid activation    \n",
    "        x = self.sigmoid(x)\n",
    "        #then remove any extra dimensions by squeezing it\n",
    "        return x.squeeze()\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        #flatten our outpot to get feature vector\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        #pass it to the first convo layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        #batch normalisation \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return torch.relu(out)\n",
    "\n",
    "def load_Training_data(file_path):\n",
    "    all_segments = []\n",
    "    #to store all features and labels that we extracted in preproccessing\n",
    "    all_labels = []\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        #iterate over each file group in the HDF5 file\n",
    "        for file_group in hf.values():\n",
    "            if isinstance(file_group, h5py.Group):\n",
    "                #load the segments from the current group and append them to thet array\n",
    "                segments = file_group['segments'][:]\n",
    "                labels = file_group['labels'][:]\n",
    "                all_segments.append(segments)\n",
    "                all_labels.append(labels)\n",
    "    #concatenate them into single numpy array\n",
    "    x = np.concatenate(all_segments, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    print(f\"Proccessed data shape: x: {x.shape}, y: {y.shape}.\")\n",
    "    print(f\"x dtype: {x.dtype}, y dtype: {y.dtype}\")\n",
    "    print(f\"x range: ({x.min()}, {x.max()}), y range: ({y.min()}, {y.max()})\")\n",
    "    #convert numpy into pytorch\n",
    "    return torch.FloatTensor(x), torch.FloatTensor(y)\n",
    "\n",
    "def load_validation_data(val_path):\n",
    "    val_data = []\n",
    "    print(f\"Loading the validation data from: {val_path}\")\n",
    "    \n",
    "    h5_file = os.path.join(val_path, 'val_all.h5')\n",
    "    #check if file exists\n",
    "    if os.path.exists(h5_file):\n",
    "        #open and iterate over the h5 file\n",
    "        with h5py.File(h5_file, 'r') as hf:\n",
    "            for key in hf.keys():\n",
    "                file_data = {'file': hf[key]['file'][()],\n",
    "                    'segments': torch.FloatTensor(hf[key]['segments'][:]),\n",
    "                    'labels': np.array(hf[key]['labels'][:]),\n",
    "                    'start_times': np.array(hf[key]['start_times'][:]),\n",
    "                    'end_times': np.array(hf[key]['end_times'][:])}\n",
    "                #APPEND EACH CREATED DICTIONARY TO VAL_DATA\n",
    "                val_data.append(file_data)\n",
    "        \n",
    "        print(f\"Loaded data from {h5_file}\")\n",
    "        print(f\"Number of validation files: {len(val_data)}\")\n",
    "    else:\n",
    "        print(f\"Couldn't find : {h5_file} file\")\n",
    "    \n",
    "    return val_data\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, device, num_epochs=3, patience=5):\n",
    "    model.train()\n",
    "    # set the model to training mode\n",
    "    #generate a maximum value for best loss we willl decrease it later\n",
    "    best_loss = float('inf')\n",
    "    #patience counter helps us to stop early incase our model's performance doesn't improve\n",
    "    patience_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "#calculate and save the loss in each epoch loop by number of epochs\n",
    "        running_loss = 0.0\n",
    "        #loop over batches from training loader\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            #move to GPU to improve performance and to avoid KERNEL failing\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            #compute loss and update model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "#after training the model we validate it using FEW SHOT LEARNING\n",
    "def validate_few_shot(model, val_data, device, thresholds=np.linspace(0, 1, 100), support_size=5, batch_size=16):\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    #lists to store results and predictions\n",
    "    all_predictions = []\n",
    "\n",
    "    for file_data in val_data:\n",
    "        #loop over validation file\n",
    "        #extract segments,labels start and end time\n",
    "        segments = file_data['segments']\n",
    "        labels = file_data['labels']\n",
    "        start_times = file_data['start_times']\n",
    "        end_times = file_data['end_times']\n",
    "        #find indices of all positive samples\n",
    "        pos_indices = np.where(labels == 1)[0]\n",
    "        #we need at least 5 samples for few shot learning\n",
    "        if len(pos_indices) < support_size:\n",
    "            print(f\"Not enough samples Skipping file: {file_data['file']}.\")\n",
    "            continue\n",
    "        #we select the first 5 positive samples as support set\n",
    "        support_indices = pos_indices[:support_size]\n",
    "        #for the rest of the samples we use them as query\n",
    "        query_indices = np.arange(len(segments))\n",
    "        query_indices = query_indices[query_indices > max(support_indices)]\n",
    "#if we don't have any query samples left for the file then skip it \n",
    "        if len(query_indices) == 0:\n",
    "            print(f\" No query samples left after the first 5 Positive in {file_data['file']} Skipping this file.\")\n",
    "            continue\n",
    "#extract support and query set\n",
    "        support_set = segments[support_indices]\n",
    "        query_set = segments[query_indices]\n",
    "        query_labels = labels[query_indices]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #extract features for support set\n",
    "            support_features = model.extract_features(support_set.to(device))\n",
    "            prototype = support_features.mean(dim=0)\n",
    "            \n",
    "            query_features = []\n",
    "            #extract the features for query set in batches \n",
    "            for i in range(0, len(query_set), batch_size):\n",
    "                batch = query_set[i:i+batch_size].to(device)\n",
    "                batch_features = model.extract_features(batch)\n",
    "                #move to cpu for distance calculation\n",
    "                query_features.append(batch_features.cpu())\n",
    "            query_features = torch.cat(query_features, dim=0)\n",
    "            #calculate distances between query and prototype\n",
    "\n",
    "            distances=torch.cdist(query_features, prototype.unsqueeze(0).cpu()).squeeze().numpy()\n",
    "            #then we normalise distances between 0 and 1\n",
    "            distances = (distances - distances.min()) / (distances.max() - distances.min())\n",
    "            \n",
    "            median_threshold = np.median(distances)\n",
    "            \n",
    "            best_threshold = 0\n",
    "            best_f1 = 0\n",
    "            #evaluation F metrics\n",
    "            for threshold in thresholds:\n",
    "                predictions = (distances < threshold).astype(int)\n",
    "                f1 = f1_score(query_labels, predictions, average='binary')\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "            \n",
    "            predictions = (distances < best_threshold).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(query_labels, predictions)\n",
    "        precision = precision_score(query_labels, predictions, average='binary', zero_division=1)\n",
    "        recall = recall_score(query_labels, predictions, average='binary', zero_division=1)\n",
    "        f1 = f1_score(query_labels, predictions, average='binary')\n",
    "#store results for each file\n",
    "        all_results.append({\n",
    "            'file': file_data['file'],\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'median_threshold': median_threshold,\n",
    "            'best_threshold': best_threshold})\n",
    "#store predictions in CSV files\n",
    "        for i, pred in enumerate(predictions):\n",
    "            if pred == 1:\n",
    "                all_predictions.append({\n",
    "                    'file': file_data['file'],\n",
    "                    'start_time': start_times[query_indices[i]],\n",
    "                    'end_time': end_times[query_indices[i]]\n",
    "                })\n",
    "\n",
    "    return all_results, all_predictions\n",
    "\n",
    "def generate_csv_output(predictions, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        #our header /cols in csv\n",
    "        writer.writerow(['Audiofilename', 'Starttime', 'Endtime'])\n",
    "        for pred in predictions:\n",
    "            #write each prediciton as rows in the csv file\n",
    "            writer.writerow([pred['file'], pred['start_time'], pred['end_time']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #use gpu if not available use cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#train based on train_all.h5 file created in preproccessing\n",
    "    train_x, train_y = load_Training_data(os.path.join(train_path, 'train_all.h5'))\n",
    "    #after training call validation data\n",
    "    val_data = load_validation_data(val_path)\n",
    "    \n",
    "    print(f\"Training data path: {train_path}.\")\n",
    "    print(f\"Validation data path: {val_path}.\")\n",
    "    print(f\"Training shape: {train_x.shape}.\")\n",
    "\n",
    "    model = AudioResNet().to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "#define loss function binary cross entropy\n",
    "    criterion = nn.BCELoss()\n",
    "    #optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #reduce the learning rate if loss doesn't keep decreasing after three epochs\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    #create a pytorch dataset and loader for trainnig the data\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    train(model, train_loader, criterion, optimizer, scheduler, device, num_epochs=15, patience=5)\n",
    "#train the model and load the best model wuth lowest loss for evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "#validate using few shot learning\n",
    "    results, predictions = validate_few_shot(model, val_data, device)\n",
    "\n",
    "    if results:\n",
    "        #if we have results print and go over them\n",
    "        print(\"Validation Results:\")\n",
    "        avg_accuracy = avg_precision = avg_recall = avg_f1 = avg_median_threshold = avg_best_threshold = 0\n",
    "        #for each file in results\n",
    "        for file_result in results:\n",
    "            print(f\"File: {file_result['file']}.\")\n",
    "            for metric, value in file_result.items():\n",
    "                if metric not in ['file', 'median_threshold', 'best_threshold']:\n",
    "                    print(f\"{metric}: {value:.4f}\")    \n",
    "                elif metric in ['median_threshold', 'best_threshold']:\n",
    "                    print(f\"{metric}: {value:.6f}\")\n",
    "            print()\n",
    "            avg_accuracy += file_result['accuracy']\n",
    "            avg_precision += file_result['precision']\n",
    "            avg_recall += file_result['recall']\n",
    "            avg_f1 += file_result['f1']\n",
    "            avg_median_threshold += file_result['median_threshold']\n",
    "            avg_best_threshold += file_result['best_threshold']\n",
    "        \n",
    "        \n",
    "        print(\"Average Results:\")\n",
    "        print(f\"Accuracy: {avg_accuracy / len(results):.4f}\")\n",
    "        print(f\"Precision: {avg_precision / len(results):.4f}\")\n",
    "        print(f\"Recall: {avg_recall / len(results):.4f}\")\n",
    "        print(f\"F1 Score: {avg_f1 / len(results):.4f}\")\n",
    "        print(f\"Median Threshold: {avg_median_threshold / len(results):.6f}\")\n",
    "        print(f\"Best Threshold: {avg_best_threshold / len(results):.6f}\")\n",
    "\n",
    "        generate_csv_output(predictions, 'CNN_ResNet.csv')\n",
    "    else:\n",
    "        print(\"No validation results were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759590c1-b0cb-4c86-b80a-092e99bc67ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
